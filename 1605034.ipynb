{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SEED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toy_test = open(\"data/toy/testNN.txt\", \"r\")\n",
    "toy_train = open(\"data/toy/trainNN.txt\", \"r\")\n",
    "\n",
    "# loop until end of file\n",
    "test_input = []\n",
    "train_input = []\n",
    "\n",
    "for line in toy_test:\n",
    "    # split line into words\n",
    "    words = line.split()\n",
    "    # convert words to float\n",
    "    for i in range(len(words)):\n",
    "        words[i] = float(words[i])\n",
    "    # append words to list\n",
    "    test_input.append(words)\n",
    "\n",
    "for line in toy_train:\n",
    "    # split line into words\n",
    "    words = line.split()\n",
    "    # convert words to float\n",
    "    for i in range(len(words)):\n",
    "        words[i] = float(words[i])\n",
    "    # append words to list\n",
    "    train_input.append(words)\n",
    "\n",
    "\n",
    "# split X and Y\n",
    "test_X = []\n",
    "test_Y = []\n",
    "train_X = []\n",
    "train_Y = []\n",
    "\n",
    "# take last column as Y\n",
    "for i in range(len(test_input)):\n",
    "    test_Y.append(int(test_input[i][-1]))\n",
    "    test_X.append(test_input[i][:-1])\n",
    "\n",
    "for i in range(len(train_input)):\n",
    "    train_Y.append(int(train_input[i][-1]))\n",
    "    train_X.append(train_input[i][:-1])\n",
    "\n",
    "\n",
    "# normalize with mean and std\n",
    "train_X = (train_X - np.mean(train_X, axis=0)) / np.std(train_X, axis=0)\n",
    "test_X = (test_X - np.mean(test_X, axis=0)) / np.std(test_X, axis=0)\n",
    "\n",
    "num_of_class = len(set(test_Y))\n",
    "num_of_input_features = len(test_X[0])\n",
    "\n",
    "\n",
    "def one_hot(label):\n",
    "    y = np.zeros((num_of_class,1))\n",
    "    y[label] = [1]\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for i in range(len(train_X)):\n",
    "    training_data.append( (np.array(train_X[i]).reshape(num_of_input_features,1), np.array(one_hot(train_Y[i]-1)) ) )\n",
    "\n",
    "\n",
    "test_data = []\n",
    "\n",
    "for i in range(len(train_X)):\n",
    "    test_data.append( (np.transpose(test_X[i]).reshape(num_of_input_features,1), np.array(one_hot(test_Y[i]-1)) ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = open(\"data/MNIST/mnist_test.csv\", \"r\")\n",
    "mnist_train = open(\"data/MNIST/mnist_train.csv\", \"r\")\n",
    "\n",
    "# pandas dataframe\n",
    "mnist_test_df = pd.read_csv(mnist_test)\n",
    "mnist_train_df = pd.read_csv(mnist_train)\n",
    "\n",
    "## make dataset smaller\n",
    "\n",
    "# take first 5000 rows of test and train\n",
    "mnist_test_df = mnist_test_df.iloc[:1000,:]\n",
    "mnist_train_df = mnist_train_df.iloc[:5000,:]\n",
    "\n",
    "\n",
    "mnist_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_input_features = 28*28\n",
    "num_of_class = 10\n",
    "\n",
    "training_data = []\n",
    "\n",
    "\n",
    "\n",
    "test_data = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.warnings.filterwarnings('ignore', 'overflow')\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def relu_prime(z):\n",
    "    z[z<=0] = 0\n",
    "    z[z>0] = 1\n",
    "    return z\n",
    "    \n",
    "class Network(object):\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        np.random.seed(42)\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for layer, w, b in zip(range(self.num_layers),self.weights, self.biases):\n",
    "            if layer == 0:\n",
    "                a = relu(np.matmul(w, a) + b)\n",
    "            else:\n",
    "                a = sigmoid(np.matmul(w, a) + b) \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def train(self, training_data, epochs, mini_batch_size, eta, test_data):\n",
    "        n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        evaluates = []\n",
    "        for j in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.gradient_descent(mini_batch, eta)\n",
    "           \n",
    "\n",
    "            # evaluate the model\n",
    "            evaluate = self.evaluate(test_data)\n",
    "            evaluates.append(evaluate)\n",
    "            print(f'Epoch {j}: {evaluate} / {n_test}')\n",
    "            \n",
    "\n",
    "    \n",
    "    def gradient_descent(self, mini_batch, eta):\n",
    "        delJdelB = [np.zeros(b.shape) for b in self.biases]\n",
    "        delJdelW = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_delJdelB, delta_delJdelW = self.backpropagation(x, y)\n",
    "            delJdelB = [nb+dnb for nb, dnb in zip(delJdelB, delta_delJdelB)]\n",
    "            delJdelW = [nw+dnw for nw, dnw in zip(delJdelW, delta_delJdelW)]\n",
    "            \n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, delJdelW)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, delJdelB)]\n",
    "        \n",
    "    \n",
    "    def backpropagation(self, x, y):\n",
    "        # initialize \n",
    "        delJdelB = [np.zeros(b.shape) for b in self.biases]\n",
    "        delJdelW = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        \n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        z_vector = []\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.matmul(w, activation) + b\n",
    "            z_vector.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "            \n",
    "        delta = sigmoid_prime(z_vector[-1]) * self.cost_derivative(activations[-1], y) \n",
    "        delJdelB[-1] = delta\n",
    "        delJdelW[-1] = np.matmul(delta, activations[-2].T)\n",
    "        \n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = z_vector[-l]\n",
    "            sp = relu_prime(z)\n",
    "            delta = np.matmul(self.weights[-l+1].T, delta) * sp\n",
    "            delJdelB[-l] = delta\n",
    "            delJdelW[-l] = np.matmul(delta, activations[-l-1].T)\n",
    "        return (delJdelB, delJdelW)\n",
    "            \n",
    "                                    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for x, y in test_data]\n",
    "        return sum([int(x == y) for x, y in test_results])\n",
    "    \n",
    "    def cost_function(self, a, y):\n",
    "        return np.sum(np.power((a - y), 2))\n",
    "    \n",
    "    \n",
    "    def cost_derivative(self, activated_output, y):\n",
    "        return 2 * (activated_output - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Network([num_of_input_features, 16, 16, num_of_class])\n",
    "# net.train(training_data, 10, 10, 2, test_data)\n",
    "\n",
    "'''\n",
    "Conv 6 5 1 2 ReLU\n",
    "Pool 2 2 Conv 12 5 1 0 ReLU\n",
    "Pool 2 2\n",
    "Conv 100 5 1 0 ReLU\n",
    "FC 10\n",
    "Softmax\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(object):\n",
    "\n",
    "    '''\n",
    "    input : height, width, depth, stride, activation_function\n",
    "    '''\n",
    "    \n",
    "\n",
    "    def __init__(self,out_depth, filter_dim, stride, padding) :\n",
    "        self.out_depth = out_depth\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        np.random.seed(42)\n",
    "        self.filtes = np.random.randn(out_depth, filter_dim, filter_dim) / np.sqrt(filter_dim**2)    \n",
    "        \n",
    "\n",
    "    def forward(self,input) :\n",
    "        \n",
    "        # padding input\n",
    "        input_padded = np.pad(input, (self.padding,self.padding), 'constant',constant_values=0)\n",
    "\n",
    "        self.input = input_padded\n",
    "        self.input_shape = input.shape\n",
    "\n",
    "        output_height = int((self.input_shape[0] - self.filter_dim ) / self.stride) + 1\n",
    "        output_width = int((self.input_shape[1] - self.filter_dim ) / self.stride) + 1\n",
    "\n",
    "        output = np.zeros((output_height, output_width, self.out_depth))\n",
    "\n",
    "        \n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                for k in range(self.out_depth):\n",
    "                    im = self.input[i*self.stride:i*self.stride+self.filter_dim, j*self.stride:j*self.stride+self.filter_dim]\n",
    "                    output[i,j,k] = np.sum(im * self.filtes[k])\n",
    "           \n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class MaxPool:\n",
    "\n",
    "    def __init__(self,height,width):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "    \n",
    "    def forward(self,input):\n",
    "        self.input = input\n",
    "\n",
    "        output_height = int(self.input.shape[0] / self.height)\n",
    "        output_width = int(self.input.shape[1] / self.width)\n",
    "\n",
    "        output = np.zeros((output_height, output_width, self.input.shape[2]))\n",
    "\n",
    "\n",
    "        # convolution\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                for k in range(self.input.shape[2]):\n",
    "                    output[i,j,k] = np.max(self.input[i*self.height:i*self.height+self.height, j*self.width:j*self.width+self.width, k])\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class FC:\n",
    "    \n",
    "    # fully connected layer\n",
    "\n",
    "    def __init__(self,out_dim) :\n",
    "        self.out_dim = out_dim\n",
    "    \n",
    "    def flatten(self,input) :\n",
    "        input_shape = input.shape\n",
    "        # convert a series of convolutional filters into a single column vector\n",
    "        return input.reshape(input_shape[0] * input_shape[1] * input_shape[2])\n",
    "\n",
    "    def forward(self,input) :\n",
    "        self.input = input\n",
    "        flattened_input = self.flatten(input)\n",
    "        np.random.seed(42)\n",
    "        self.weights = np.random.randn(self.out_dim, flattened_input.shape[0]) / np.sqrt(flattened_input.shape[0])\n",
    "        self.biases = np.random.randn(self.out_dim) / np.sqrt(flattened_input.shape[0])\n",
    "\n",
    "        self.z = np.matmul(self.weights, flattened_input) + self.biases\n",
    "\n",
    "        return self.z\n",
    "\n",
    "        \n",
    "\n",
    "class ReLU:\n",
    "    \n",
    "    def forward(self,input) :\n",
    "        self.input = input\n",
    "        return np.maximum(0,input)\n",
    "    \n",
    "    def backward(self,input) :\n",
    "        return 1 * (input > 0)\n",
    "    \n",
    "\n",
    "def Softmax(input) :\n",
    "    exp = np.exp(input)\n",
    "    return exp / np.sum(exp, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "class CNN :\n",
    "\n",
    "    def __init__(self,file_name) :\n",
    "        # open a text file in read mode\n",
    "        file = open(file_name, \"r\")\n",
    "        self.lines = file.readlines()\n",
    "          \n",
    "    \n",
    "\n",
    "    def forward(self,input) :\n",
    "\n",
    "        self.output_layer = []\n",
    "        self.output_layer.append(input)\n",
    "\n",
    "        for line in self.lines :\n",
    "            words = line.split()\n",
    "            print(words)\n",
    "\n",
    "            if words[0] == 'Conv' :\n",
    "                conv = ConvNet(int(words[1]), int(words[2]), int(words[3]), int(words[4]))\n",
    "                out = conv.forward(self.output_layer[-1])\n",
    "                self.output_layer.append(out)\n",
    "            \n",
    "            elif words[0] == 'Pool' :\n",
    "                pool = MaxPool(int(words[1]), int(words[2]))\n",
    "                out = pool.forward(self.output_layer[-1])\n",
    "                self.output_layer.append(out)\n",
    "\n",
    "            elif words[0] == 'FC' :\n",
    "                fc = FC(int(words[1]))\n",
    "                out = fc.forward(self.output_layer[-1])\n",
    "                self.output_layer.append(out)\n",
    "            \n",
    "            elif words[0] == 'ReLU' :\n",
    "                relu = ReLU()\n",
    "                out = relu.forward(self.output_layer[-1])\n",
    "                self.output_layer.append(out)\n",
    "            \n",
    "            elif words[0] == 'Softmax' :\n",
    "                out = Softmax(self.output_layer[-1])\n",
    "                self.output_layer.append(out)\n",
    "            \n",
    "            else :\n",
    "                print('Error')\n",
    "        \n",
    "        return self.output_layer[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conv', '6', '5', '1', '2']\n",
      "['ReLU']\n",
      "['Pool', '2', '2']\n",
      "['Conv', '12', '5', '1', '0']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,5,6) (5,5) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-85acef6810b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-08150e8185eb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Conv'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-08150e8185eb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiltes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,5,6) (5,5) "
     ]
    }
   ],
   "source": [
    "myNet = CNN('data/archi.txt')\n",
    "\n",
    "\n",
    "image = np.random.randn(28,28)\n",
    "\n",
    "out = myNet.forward(image)\n",
    "\n",
    "print(out.shape)\n",
    "print(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 24, 6)\n",
      "(12, 12, 6)\n",
      "[ 0.32416408  1.80284466  0.04520829 -0.94732839 -1.30551966 -0.09391854\n",
      " -2.29825827 -0.43230547  0.6015653   0.98847814]\n"
     ]
    }
   ],
   "source": [
    "# a sample 28x28 3 channel image\n",
    "\n",
    "\n",
    "\n",
    "out1 = ConvNet(6,5,1,2).forward(image)\n",
    "out2 = MaxPool(2,2).forward(out1)\n",
    "out3 = FC(10).forward(out2)\n",
    "\n",
    "print(out1.shape)\n",
    "print(out2.shape)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a6f8c3de90b578c42c5dca1ace0652505f65d16b2ece0eaf147a9cf03f55f9e2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
